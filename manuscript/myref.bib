@techreport{minka2004power,
author = {Minka, Tom},
title = {Power EP},
year = {2004},
month = {January},
abstract = {This note describes power EP, an extension of Expectation Propagation (EP) that makes the computations more tractable. In this way, power EP is applicable to a wide variety of models, much more than EP. Instead of minimizing KL-divergence at each step, power EP minimizes α-divergence. This minimization turns out to be equivalent to minimizing KL-divergence with the exact distribution raised to a power. By choosing this power to cancel exponents, the problem may be substantially simplified. The resulting approximation is not the same as regular EP, but in practice is still very good, and allows tackling problems which are intractable under regular EP.},
url = {https://www.microsoft.com/en-us/research/publication/power-ep/},
pages = {6},
number = {MSR-TR-2004-149},
}
@ARTICLE{cespedes2014mimo, 
author={J. {Céspedes} and P. M. {Olmos} and M. {Sánchez-Fernández} and F. {Perez-Cruz}}, 
journal={IEEE Transactions on Communications}, 
title={Expectation Propagation Detection for High-Order High-Dimensional MIMO Systems}, 
year={2014}, 
volume={62}, 
number={8}, 
pages={2840-2849}, 
keywords={antenna arrays;computational complexity;iterative methods;maximum likelihood detection;MIMO communication;quadrature amplitude modulation;expectation propagation detection;high-order high-dimensional MIMO systems;communication system;multiple-input multiple-output constellation;high-order QAM constellation;spectral efficiency maximization;antenna number;low-complexity MIMO receivers;MIMO receiver efficiency;symbol detection;maximum likelihood detection;sphere-decoding method;transmitter-receiver number;low-complexity high-accuracy MIMO symbol detector;expectation propagation algorithm;EP algorithm;iterative approximation;polynomial-time;transmitted symbol posterior distribution;EP MIMO detector;symbol error rate reduction;computational complexity reduction;MIMO;Detectors;Approximation methods;Vectors;Signal to noise ratio;Computational complexity;High-dimensional MIMO communication systems;high-order QAM;low complexity;expectation propagation}, 
doi={10.1109/TCOMM.2014.2332349}, 
ISSN={0090-6778}, 
month={Aug},}
@ARTICLE\{opper2005ec,
    author       = "M. Opper and O. Winther",
    title        = "Expectation Consistent Approximate Inference",
    year         = "2005",
    pages        = "2177-2204",
    journal      = "Journal of Machine Learning Research",
    volume       = "6",
    editor       = "",
    number       = "",
    publisher    = "",
    url          = "http://localhost/pubdb/p.php?3460",
    abstract     = "We propose a novel framework for approximations to intractable probabilistic models which is based on a free energy formulation. The approximation can be understood from replacing an average over the original intractable distribution with a tractable one. It requires two tractable probability distributions which are made consistent on a set of moments and encode different features of the original intractable distribution. In this way we are able to use Gaussian approximations for models with discrete or bounded variables which allow us to include non-trivial correlations which are neglected in many other methods.
We test the framework on toy benchmark problems for binary variables on fully connected graphs and {2D} grids and compare with other methods, such as loopy belief propagation. Good performance is already achieved by using single nodes as tractable substructures. Significant improvements are obtained when a spanning tree is used instead."
}
@ARTICLE{cespedes2018ecmimo, 
author={J. {Céspedes} and P. M. {Olmos} and M. {Sánchez-Fernández} and F. {Perez-Cruz}}, 
journal={IEEE Transactions on Vehicular Technology}, 
title={Probabilistic MIMO Symbol Detection With Expectation Consistency Approximate Inference}, 
year={2018}, 
volume={67}, 
number={4}, 
pages={3481-3494}, 
keywords={antenna arrays;approximation theory;channel coding;concave programming;MIMO communication;parity check codes;probability;radiofrequency interference;transmitting antennas;probabilistic MIMO symbol detection;expectation consistency approximate inference;low-complexity probabilistic algorithms;soft symbol detection;expectation consistency framework;approximate inference problem;nonconvex function;belief propagation;expectation propagation;MIMO symbol detection problem;EC function;EC MIMO detector;probabilistic output;low-density parity-check channel code;high-dimensional multiple-input multiple-output systems;input-output mutual information;complexity order cubic;transmitting antennas;Multiple-input multiple-output (MIMO) communication systems;approximate inference;expectation consistency (EC);low-density parity-check (LDPC) codes}, 
doi={10.1109/TVT.2017.2786638}, 
ISSN={0018-9545}, 
month={April},}
@ARTICLE{2017arXiv170104862A,
   author = {{Arjovsky}, M. and {Bottou}, L.},
    title = "{Towards Principled Methods for Training Generative Adversarial Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1701.04862},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Learning},
     year = 2017,
    month = jan,
   adsurl = {http://adsabs.harvard.edu/abs/{2016arXiv1610065452017arXiv170104862A},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
}
@ARTICLE{2017arXiv170100160G,
   author = {{Goodfellow}, I.},
    title = "{NIPS 2016 Tutorial: Generative Adversarial Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1701.00160},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning},
     year = 2017,
    month = dec,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170100160G},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016arXiv160602206F,
   author = {{Farnia}, F. and {Tse}, D.},
    title = "{A Minimax Approach to Supervised Learning}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1606.02206},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Information Theory, Computer Science - Learning},
     year = 2016,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160602206F},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@incollection{NIPS2015_5745,
title = {Distributionally Robust Logistic Regression},
author = {Shafieezadeh-Abadeh, Soroosh and Esfahani, Peyman Mohajerin and Kuhn, Daniel},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {1576--1584},
year = {2015},
publisher = {Curran Associates, Inc.}
}
@incollection{NIPS2014_5458,
title = {Robust Classification Under Sample Selection Bias},
author = {Liu, Anqi and Ziebart, Brian},
booktitle = {Advances in Neural Information Processing Systems 27},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {37--45},
year = {2014},
publisher = {Curran Associates, Inc.}
}
@ARTICLE{2017arXiv171010016S,
   author = {{Shafieezadeh-Abadeh}, S. and {Kuhn}, D. and {Mohajerin Esfahani}, P.
	},
    title = "{Regularization via Mass Transportation}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1710.10016},
 primaryClass = "math.OC",
 keywords = {Mathematics - Optimization and Control, Computer Science - Learning, Statistics - Machine Learning},
     year = 2017,
    month = oct,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171010016S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2013arXiv1312.6199S,
   author = {{Szegedy}, C. and {Zaremba}, W. and {Sutskever}, I. and {Bruna}, J. and 
	{Erhan}, D. and {Goodfellow}, I. and {Fergus}, R.},
    title = "{Intriguing properties of neural networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1312.6199},
 primaryClass = "cs.CV",
 keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
     year = 2013,
    month = dec,
   adsurl = {http://adsabs.harvard.edu/abs/2013arXiv1312.6199S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2017arXiv171100970S,
   author = {{Santurkar}, S. and {Schmidt}, L. and {M{\c a}dry}, A.},
    title = "{A Classification-Based Study of Covariate Shift in GAN Distributions}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1711.00970},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
     year = 2017,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171100970S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@incollection{NIPS2016_6248,
title = {Wasserstein Training of Restricted Boltzmann Machines},
author = {Montavon, Gr\'{e}goire and M\"{u}ller, Klaus-Robert and Cuturi, Marco},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {3718--3726},
year = {2016},
publisher = {Curran Associates, Inc.}
}
@Inbook{Hinton2012,
author="Hinton, Geoffrey E.",
editor="Montavon, Gr{\'e}goire
and Orr, Genevi{\`e}ve B.
and M{\"u}ller, Klaus-Robert",
title="A Practical Guide to Training Restricted Boltzmann Machines",
bookTitle="Neural Networks: Tricks of the Trade: Second Edition",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="599--619",
abstract="Restricted Boltzmann machines (RBMs) have been used as generative models of many different types of data. RBMs are usually trained using the contrastive divergence learning procedure. This requires a certain amount of practical experience to decide how to set the values of numerical meta-parameters. Over the last few years, the machine learning group at the University of Toronto has acquired considerable expertise at training RBMs and this guide is an attempt to share this expertise with other machine learning researchers.",
isbn="978-3-642-35289-8",
doi="10.1007/978-3-642-35289-8_32",
url="https://doi.org/10.1007/978-3-642-35289-8_32"
}
@InProceedings{2013arXiv1310.4375C,
  title = 	 {Fast Computation of Wasserstein Barycenters},
  author = 	 {Marco Cuturi and Arnaud Doucet},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {685--693},
  year = 	 {2014},
  volume = 	 {32},
  number =       {2},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/cuturi14.pdf},
  url = 	 {http://proceedings.mlr.press/v32/cuturi14.html},
  abstract = 	 {We present new algorithms to compute the mean of a set of N empirical probability measures under the optimal transport metric. This mean, known as the Wasserstein barycenter \citepagueh2011barycenters,rabin2012, is the measure that minimizes the sum of its Wasserstein distances to each element in that set. We argue through a simple example that Wasserstein barycenters have appealing properties that differentiate them from other barycenters proposed recently, which all build on kernel smoothing and/or Bregman divergences. Two original algorithms are proposed that require the repeated computation of primal and dual optimal solutions of transport problems. However direct implementation of these algorithms is too costly as optimal transports are notoriously computationally expensive. Extending the work of \citetcuturi2013sinkhorn, we smooth both the primal and dual of the optimal transport problem to recover fast approximations of the primal and dual optimal solutions. We apply these algorithms to the visualization of perturbed images and to a clustering problem.}
}
@incollection{2013arXiv1306.0895C,
title = {Sinkhorn Distances: Lightspeed Computation of Optimal Transport},
author = {Cuturi, Marco},
booktitle = {Advances in Neural Information Processing Systems 26},
editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
pages = {2292--2300},
year = {2013},
publisher = {Curran Associates, Inc.}
}
@ARTICLE{2018arXiv180607066M,
   author = {{Montufar}, G.},
    title = "{Restricted Boltzmann Machines: Introduction and Review}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1806.07066},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Computer Science - Information Theory, Mathematics - Probability, Mathematics - Statistics Theory, Statistics - Machine Learning},
     year = 2018,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180607066M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{2017arXiv170107875A,
  title={Wasserstein generative adversarial networks},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  booktitle={International Conference on Machine Learning},
  pages={214--223},
  year={2017}
}
@INPROCEEDINGS{1467314, 
author={S. Chopra and R. Hadsell and Y. LeCun}, 
booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)}, 
title={Learning a similarity metric discriminatively, with application to face verification}, 
year={2005}, 
volume={1}, 
number={}, 
pages={539-546 vol. 1}, 
keywords={face recognition;learning (artificial intelligence);similarity metric learning;face verification;face recognition;L/sub 1/ norm;semantic distance approximation;discriminative loss function;geometric distortion;Character generation;Drives;Robustness;System testing;Spatial databases;Glass;Artificial neural networks;Support vector machines;Support vector machine classification;Face recognition}, 
doi={10.1109/CVPR.2005.202}, 
ISSN={1063-6919}, 
month={June},}
}
@incollection{2017arXiv170400028G,
title = {Improved Training of Wasserstein GANs},
author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and others},
booktitle = {Advances in Neural Information Processing Systems 30},
pages = {5767--5777},
year = {2017},
publisher = {Curran Associates, Inc.}
}
@ARTICLE{2017arXiv171105084C,
   author = {{Cao}, G. and {Yang}, Y. and {Lei}, J. and others},
    title = "{TripletGAN: Training Generative Model with Triplet Loss}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1711.05084},
 keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
     year = 2017,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171105084C},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@incollection{NIPS2014_5423,
title = {Generative Adversarial Nets},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems 27},
pages = {2672--2680},
year = {2014},
publisher = {Curran Associates, Inc.}
}
@book{villani2003topics,
  title={Topics in optimal transportation},
  author={Villani, C{\'e}dric},
  number={58},
  year={2003},
  publisher={American Mathematical Soc.}
}
@INPROCEEDINGS{7298682, 
author={F. Schroff and D. Kalenichenko and J. Philbin}, 
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={FaceNet: A unified embedding for face recognition and clustering}, 
year={2015}, 
volume={}, 
number={}, 
pages={815-823}, 
keywords={convolution;data mining;face recognition;image matching;neural nets;optimisation;pattern clustering;FaceNet embedding;face recognition;face clustering;deep convolutional network;embedding optimization;face patch matching;online triplet mining method;Face;Face recognition;Training;Accuracy;Artificial neural networks;Standards;Principal component analysis}, 
doi={10.1109/CVPR.2015.7298682}, 
ISSN={1063-6919}, 
month={June},}
@ARTICLE{2018arXiv180607755X,
   author = {{Xu}, Q. and {Huang}, G. and {Yuan}, Y. and {Guo}, C. and {Sun}, Y. and 
	{Wu}, F. and {Weinberger}, K.},
    title = "{An empirical study on evaluation metrics of generative adversarial networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1806.07755},
 keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
     year = 2018,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180607755X},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2016arXiv161006545L,
   author = {{Lopez-Paz}, D. and {Oquab}, M.},
    title = {Revisiting Classifier Two-Sample Tests},
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1610.06545},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning},
     year = 2016,
    month = oct,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161006545L},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{ponce2011computer,
  title={Computer vision: a modern approach},
  author={Ponce, Jean and Forsyth, David and Willow, Equipe-projet and others},
  journal={Computer},
  volume={16},
  number={11},
  year={2011}
}
@incollection{NIPS2016_6125,
title = {Improved Techniques for Training GANs},
author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and others},
booktitle = {Advances in Neural Information Processing Systems 29},
pages = {2234--2242},
year = {2016},
publisher = {Curran Associates, Inc.}
}
@ARTICLE{2018arXiv180101973B,
   author = {{Barratt}, S. and {Sharma}, R.},
    title = "{A Note on the Inception Score}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1801.01973},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
     year = 2018,
    month = jan,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180101973B},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@INPROCEEDINGS{1640964, 
author={R. Hadsell and S. Chopra and Y. LeCun}, 
booktitle={2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)}, 
title={Dimensionality Reduction by Learning an Invariant Mapping}, 
year={2006}, 
volume={2}, 
number={}, 
pages={1735-1742}, 
keywords={Extraterrestrial measurements;Image generation;Biology;Geoscience;Astronomy;Service robots;Manufacturing industries;Image analysis;Feature extraction;Data visualization}, 
doi={10.1109/CVPR.2006.100}, 
ISSN={1063-6919}, 
month={June},}

@ARTICLE{2015arXiv151106434R,
   author = {{Radford}, A. and {Metz}, L. and {Chintala}, S.},
    title = "{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1511.06434},
 keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
     year = 2015,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151106434R},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{ClaiciCS18,
  author    = {Sebastian Claici and
               Edward Chien and
               Justin Solomon},
  title     = {Stochastic Wasserstein Barycenters},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning,
               {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July
               10-15, 2018},
  pages     = {998--1007},
  year      = {2018},
  timestamp = {Fri, 13 Jul 2018 14:58:25 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@ARTICLE{2016arXiv161006519S,
   author = {{Schmitzer}, B.},
    title = "{Stabilized Sparse Scaling Algorithms for Entropy Regularized Transport Problems}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1610.06519},
 primaryClass = "math.OC",
 keywords = {Mathematics - Optimization and Control, Computer Science - Computational Engineering, Finance, and Science, Mathematics - Numerical Analysis},
     year = 2016,
    month = oct,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161006519S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@InProceedings{pmlr-v84-genevay18a,
  title = 	 {Learning Generative Models with Sinkhorn Divergences},
  author = 	 {Aude Genevay and Gabriel Peyre and Marco Cuturi},
  booktitle = 	 {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1608--1617},
  year = 	 {2018},
  volume = 	 {84},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Playa Blanca, Lanzarote, Canary Islands},
  month = 	 {09--11 Apr},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v84/genevay18a/genevay18a.pdf},
  url = 	 {http://proceedings.mlr.press/v84/genevay18a.html},
  abstract = 	 {The ability to compare two degenerate probability distributions, that is two distributions supported on low-dimensional manifolds in much higher-dimensional spaces, is a crucial factor in the estimation of generative mod- els.It is therefore no surprise that optimal transport (OT) metrics and their ability to handle measures with non-overlapping sup- ports have emerged as a promising tool. Yet, training generative machines using OT raises formidable computational and statistical challenges, because of (i) the computational bur- den of evaluating OT losses, (ii) their instability and lack of smoothness, (iii) the difficulty to estimate them, as well as their gradients, in high dimension. This paper presents the first tractable method to train large scale generative models using an OT-based loss called Sinkhorn loss which tackles these three issues by relying on two key ideas: (a) entropic smoothing, which turns the original OT loss into a differentiable and more robust quantity that can be computed using Sinkhorn fixed point iterations; (b) algorithmic (automatic) differentiation of these iterations with seam- less GPU execution. Additionally, Entropic smoothing generates a family of losses interpolating between Wasserstein (OT) and Energy distance/Maximum Mean Discrepancy (MMD) losses, thus allowing to find a sweet spot leveraging the geometry of OT on the one hand, and the favorable high-dimensional sample complexity of MMD, which comes with un- biased gradient estimates. The resulting computational architecture complements nicely standard deep network generative models by a stack of extra layers implementing the loss function.}
}
@incollection{2017arXiv170608500H,
title = {GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and others},
booktitle = {Advances in Neural Information Processing Systems 30},
pages = {6626--6637},
year = {2017},
publisher = {Curran Associates, Inc.}
}
@inproceedings{2018arXiv180205957M,
title={Spectral Normalization for Generative Adversarial Networks},
author={Takeru Miyato and Toshiki Kataoka and Masanori Koyama and Yuichi Yoshida},
booktitle={International Conference on Learning Representations},
year={2018}
}
@ARTICLE{2017arXiv170208398M,
   author = {{Mroueh}, Y. and {Sercu}, T. and {Goel}, V.},
    title = "{McGan: Mean and Covariance Feature Matching GAN}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1702.08398},
 keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
     year = 2017,
    month = feb,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170208398M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2017arXiv170502894L,
   author = {{Lim}, J.~H. and {Ye}, J.~C.},
    title = "{Geometric GAN}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1705.02894},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
     year = 2017,
    month = may,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170502894L},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018arXiv180508318Z,
   author = {{Zhang}, H. and {Goodfellow}, I. and {Metaxas}, D. and {Odena}, A.
	},
    title = "{Self-Attention Generative Adversarial Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1805.08318},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
     year = 2018,
    month = may,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180508318Z},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@misc{pytorch,
  title = {Pytorch Autograd},
  howpublished = {\url{https://pytorch.org/docs/stable/autograd.html}},
}
@misc{tensorflow,
  title = {Tensorflow Automatic differentiation},
  howpublished = {\url{https://pytorch.org/docs/stable/autograd.html}},
}
@ARTICLE{2018arXiv180300567P,
   author = {{Peyr{\'e}}, G. and {Cuturi}, M.},
    title = "{Computational Optimal Transport}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1803.00567},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning},
     year = 2018,
    month = mar,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180300567P},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@INPROCEEDINGS{5459199, 
author={O. Pele and M. Werman}, 
booktitle={2009 IEEE 12th International Conference on Computer Vision}, 
title={Fast and robust Earth Mover's Distances}, 
year={2009}, 
volume={}, 
number={}, 
pages={460-467}, 
keywords={edge detection;thresholded ground distances;flow-network;histograms;outlier noise;quantization effects;robust earth mover's distances;Robustness;Earth;Histograms;Costs;Humans;Image retrieval;Image edge detection;Quantization;Computer vision;Image databases}, 
doi={10.1109/ICCV.2009.5459199}, 
ISSN={2380-7504}, 
month={Sept},}
@ARTICLE{2017arXiv171101558T,
   author = {{Tolstikhin}, I. and {Bousquet}, O. and {Gelly}, S. and {Schoelkopf}, B.
	},
    title = "{Wasserstein Auto-Encoders}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1711.01558},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
     year = 2017,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171101558T},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2017arXiv170104722M,
   author = {{Mescheder}, L. and {Nowozin}, S. and {Geiger}, A.},
    title = "{Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1701.04722},
 keywords = {Computer Science - Machine Learning},
     year = 2017,
    month = jan,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170104722M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2017arXiv170102386T,
   author = {{Tolstikhin}, I. and {Gelly}, S. and {Bousquet}, O. and {Simon-Gabriel}, C.-J. and 
	{Sch{\"o}lkopf}, B.},
    title = "{AdaGAN: Boosting Generative Models}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1701.02386},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
     year = 2017,
    month = jan,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170102386T},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2018arXiv180601879W,
   author = {{Weed}, J.},
    title = "{An explicit analysis of the entropic penalty in linear programming}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1806.01879},
 primaryClass = "math.OC",
 keywords = {Mathematics - Optimization and Control, Computer Science - Machine Learning, Statistics - Machine Learning},
     year = 2018,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180601879W},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2018arXiv180600880K,
   author = {{Khayatkhoei}, M. and {Elgammal}, A. and {Singh}, M.},
    title = "{Disconnected Manifold Learning for Generative Adversarial Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1806.00880},
 keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
     year = 2018,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180600880K},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2018arXiv181106763L,
   author = {{Liu}, D. and {Th{\`a}nh Vu}, M. and {Chatterjee}, S. and {Rasmussen}, L.~K.
	},
    title = "{Entropy-regularized Optimal Transport Generative Models}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1811.06763},
 keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
     year = 2018,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv181106763L},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2018arXiv180703039K,
   author = {{Kingma}, D.~P. and {Dhariwal}, P.},
    title = "{Glow: Generative Flow with Invertible 1x1 Convolutions}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1807.03039},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
     year = 2018,
    month = jul,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180703039K},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2016arXiv160508803D,
   author = {{Dinh}, L. and {Sohl-Dickstein}, J. and {Bengio}, S.},
    title = "{Density estimation using Real NVP}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1605.08803},
 keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
     year = 2016,
    month = may,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160508803D},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@incollection{NIPS2016_6399,
title = {InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets},
author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {2172--2180},
year = {2016},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6399-infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets.pdf}
}
@article{bang2018icml,
  author    = {Duhyeon Bang and
               Hyunjung Shim},
  title     = {Improved Training of Generative Adversarial Networks Using Representative
               Features},
  journal   = {CoRR},
  volume    = {abs/1801.09195},
  year      = {2018},
  archivePrefix = {arXiv},
  eprint    = {1801.09195},
  timestamp = {Mon, 13 Aug 2018 16:47:52 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1801-09195},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@incollection{eitan2018nips_gmm,
title = {On GANs and GMMs},
author = {Richardson, Eitan and Weiss, Yair},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {5852--5863},
year = {2018},
publisher = {Curran Associates, Inc.}
}
@article{grover2017aaai_boost,
  author    = {Aditya Grover and
               Stefano Ermon},
  title     = {Boosted Generative Models},
  journal   = {CoRR},
  volume    = {abs/1702.08484},
  year      = {2017},
  archivePrefix = {arXiv},
  eprint    = {1702.08484},
  timestamp = {Mon, 13 Aug 2018 16:46:31 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/GroverE17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/DinhKB14,
  author    = {Laurent Dinh and
               David Krueger and
               Yoshua Bengio},
  title     = {{NICE:} Non-linear Independent Components Estimation},
  journal   = {CoRR},
  volume    = {abs/1410.8516},
  year      = {2014},
  archivePrefix = {arXiv},
  eprint    = {1410.8516},
  timestamp = {Mon, 13 Aug 2018 16:48:00 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/DinhKB14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/GhoshKNTD17,
  author    = {Arnab Ghosh and
               Viveka Kulharia and
               Vinay P. Namboodiri and
               Philip H. S. Torr and
               Puneet Kumar Dokania},
  title     = {Multi-Agent Diverse Generative Adversarial Networks},
  journal   = {CoRR},
  volume    = {abs/1704.02906},
  year      = {2017},
  archivePrefix = {arXiv},
  eprint    = {1704.02906},
  timestamp = {Mon, 13 Aug 2018 16:47:13 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/GhoshKNTD17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{hoang2018mgan,
title={{MGAN}: Training Generative Adversarial Nets with Multiple Generators},
author={Quan Hoang and Tu Dinh Nguyen and Trung Le and Dinh Phung},
booktitle={International Conference on Learning Representations},
year={2018}
}
@inproceedings{
donahue2017adversarial,
title={Adversarial Feature Learning},
author={Jeff Donahue and Philipp Krähenbühl and Trevor Darrell},
booktitle={International Conference on Learning Representations},
year={2017},
}
@inproceedings{
dumoulin2017adversarially,
title={Adversarial Learned Inference},
author={Vincent Dumoulin and Ishmael Belghazi and Ben Poole and Olivier Mastropietro and Alex Lamb and Martin Arjovsky and Aaron Courville},
booktitle={International Conference on Learning Representations},
year={2017},
}
@incollection{dustin2017hierarchical,
title = {Hierarchical Implicit Models and Likelihood-Free Variational Inference},
author = {Tran, Dustin and Ranganath, Rajesh and Blei, David},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5523--5533},
year = {2017},
publisher = {Curran Associates, Inc.}
}
@inproceedings{
salimans2018improving,
title={Improving {GAN}s Using Optimal Transport},
author={Tim Salimans and Han Zhang and Alec Radford and Dimitris Metaxas},
booktitle={International Conference on Learning Representations},
year={2018}
}
@INPROCEEDINGS{ledig2017photo, 
author={C. Ledig and L. Theis and F. Huszár and J. Caballero and A. Cunningham and A. Acosta and A. Aitken and A. Tejani and J. Totz and Z. Wang and W. Shi}, 
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network}, 
year={2017}, 
volume={}, 
number={}, 
pages={105-114}, 
keywords={feedforward neural nets;image reconstruction;image resolution;image sampling;image texture;realistic images;upscaling factors;perceptual loss function;adversarial loss;content loss;natural image manifold;discriminator network;perceptual similarity;deep residual network;photo-realistic textures;heavily downsampled images;SRGAN;high-resolution images;photo-realistic single image super-resolution;generative adversarial network;deeper convolutional neural networks;super-resolution methods;objective function;mean squared reconstruction error;signal-to-noise ratios;photo-realistic natural images;Image resolution;Signal resolution;Gallium nitride;Image reconstruction;Manifolds;Training;Network architecture}, 
doi={10.1109/CVPR.2017.19}, 
ISSN={1063-6919}, 
month={July},}
@book{Bishop:2006:PRM:1162264,
 author = {Bishop, Christopher M.},
 title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
 year = {2006},
 isbn = {0387310738},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
} 
@ARTICLE{ma2011bayesian, 
author={Z. Ma and A. Leijon}, 
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
title={Bayesian Estimation of Beta Mixture Models with Variational Inference}, 
year={2011}, 
volume={33}, 
number={11}, 
pages={2160-2173}, 
keywords={approximation theory;Bayes methods;expectation-maximisation algorithm;inference mechanisms;parameter estimation;beta mixture models;Bayesian parameter estimation;posterior distribution;variational inference framework;extended factorized approximation method;model complexity;expectation maximization algorithm;Approximation methods;Bayesian methods;Data models;Numerical models;Maximum likelihood estimation;Probability density function;Bayesian estimation;maximum likelihood estimation;beta distribution;mixture modeling;variational inference;factorized approximation.;Algorithms;Bayes Theorem;Computer Simulation;Engineering;Humans;Likelihood Functions;Pattern Recognition, Automated;Skin Pigmentation}, 
doi={10.1109/TPAMI.2011.63}, 
ISSN={0162-8828}, 
month={Nov},}
@article{dempster1977maximum,
  title={Maximum likelihood from incomplete data via the EM algorithm},
  author={Dempster, Arthur P and Laird, Nan M and Rubin, Donald B},
  journal={Journal of the royal statistical society. Series B (methodological)},
  pages={1--38},
  year={1977},
  publisher={JSTOR}
}
@article{DBLP:journals/corr/KingmaB14,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  journal   = {CoRR},
  volume    = {abs/1412.6980},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.6980},
  archivePrefix = {arXiv},
  eprint    = {1412.6980},
  timestamp = {Mon, 13 Aug 2018 16:47:35 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/KingmaB14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{kurutach2018learning_plan,
  author    = {Thanard Kurutach and
               Aviv Tamar and
               Ge Yang and
               Stuart J. Russell and
               Pieter Abbeel},
  title     = {Learning Plannable Representations with Causal InfoGAN},
  journal   = {CoRR},
  volume    = {abs/1807.09341},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.09341},
  archivePrefix = {arXiv},
  eprint    = {1807.09341},
  timestamp = {Mon, 13 Aug 2018 16:48:44 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1807-09341},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@TECHREPORT{Zhu95informationgeometric,
    author = {Huaiyu Zhu and Richard Rohwer},
    title = {Information Geometric Measurements of Generalisation},
    institution = {},
    year = {1995}
}
@techreport{divergence-measures-and-message-passing,
author = {Minka, Tom},
title = {Divergence Measures and Message Passing},
year = {2005},
month = {January},
abstract = {

This paper presents a unifying view of message-passing algorithms, as methods to approximate a complex Bayesian network by a simpler network with minimum information divergence. In this view, the difference between mean-field methods and belief propagation is not the amount of structure they model, but only the measure of loss they minimize (`exclusive' versus `inclusive' Kullback-Leibler divergence). In each case, message-passing arises by minimizing a localized version of the divergence, local to each factor. By examining these divergence measures, we can intuit the types of solution they prefer (symmetry-breaking, for example) and their suitability for different tasks. Furthermore, by considering a wider variety of divergence measures (such as alpha-divergences), we can achieve different complexity and performance goals.


},
url = {https://www.microsoft.com/en-us/research/publication/divergence-measures-and-message-passing/},
pages = {17},
number = {MSR-TR-2005-173},
}
@INPROCEEDINGS{pseudo_priorBP2010, 
author={J. {Goldberger} and A. {Leshem}}, 
booktitle={2010 IEEE Information Theory Workshop on Information Theory (ITW 2010, Cairo)}, 
title={Pseudo Prior Belief Propagation for densely connected discrete graphs}, 
year={2010}, 
volume={}, 
number={}, 
pages={1-5}, 
keywords={maximum likelihood estimation;mean square error methods;MIMO communication;pseudo prior belief propagation;densely connected discrete graphs;linear least squares problem;maximum likelihood;minimum mean square error detection;MIMO detection problem;computational complexity;Belief propagation;MIMO;Receiving antennas;Vectors;Least squares methods;Transmitting antennas;Maximum likelihood detection;Mean square error methods;Graphical models;Application software}, 
doi={10.1109/ITWKSPS.2010.5503198}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{zhang2013denoise, 
author={R. {Zhang} and C. A. {Bouman} and J. {Thibault} and K. D. {Sauer}}, 
booktitle={2013 IEEE Global Conference on Signal and Information Processing}, 
title={Gaussian mixture Markov random field for image denoising and reconstruction}, 
year={2013}, 
volume={}, 
number={}, 
pages={1089-1092}, 
keywords={Gaussian processes;image denoising;image reconstruction;Markov processes;maximum likelihood estimation;mixture models;optimisation;quadratic optimization;MAP estimates;image patches;global image model;tomographic reconstruction;inverse problem;GM-MRF;image reconstruction;image denoising;Markov random field;Gaussian mixture;Computational modeling;Image reconstruction;PSNR;Noise reduction;Optimization;Computed tomography;Materials;Markov random fields;Gaussian mixture;patch-based methods;image model;prior model}, 
doi={10.1109/GlobalSIP.2013.6737083}, 
ISSN={}, 
month={Dec},}
@ARTICLE{cespedes2014ep, 
author={J. {Céspedes} and P. M. {Olmos} and M. {Sánchez-Fernández} and F. {Perez-Cruz}}, 
journal={IEEE Transactions on Communications}, 
title={Expectation Propagation Detection for High-Order High-Dimensional MIMO Systems}, 
year={2014}, 
volume={62}, 
number={8}, 
pages={2840-2849}, 
keywords={antenna arrays;computational complexity;iterative methods;maximum likelihood detection;MIMO communication;quadrature amplitude modulation;expectation propagation detection;high-order high-dimensional MIMO systems;communication system;multiple-input multiple-output constellation;high-order QAM constellation;spectral efficiency maximization;antenna number;low-complexity MIMO receivers;MIMO receiver efficiency;symbol detection;maximum likelihood detection;sphere-decoding method;transmitter-receiver number;low-complexity high-accuracy MIMO symbol detector;expectation propagation algorithm;EP algorithm;iterative approximation;polynomial-time;transmitted symbol posterior distribution;EP MIMO detector;symbol error rate reduction;computational complexity reduction;MIMO;Detectors;Approximation methods;Vectors;Signal to noise ratio;Computational complexity;High-dimensional MIMO communication systems;high-order QAM;low complexity;expectation propagation}, 
doi={10.1109/TCOMM.2014.2332349}, 
ISSN={0090-6778}, 
month={Aug},}
@ARTICLE{kschischang2001factor_graph, 
author={F. R. {Kschischang} and B. J. {Frey} and H. -. {Loeliger}}, 
journal={IEEE Transactions on Information Theory}, 
title={Factor graphs and the sum-product algorithm}, 
year={2001}, 
volume={47}, 
number={2}, 
pages={498-519}, 
keywords={graph theory;message passing;functional analysis;artificial intelligence;signal processing;digital communication;iterative decoding;Viterbi decoding;belief networks;Kalman filters;fast Fourier transforms;hidden Markov models;turbo codes;sum-product algorithm;factor graphs;global functions;local functions;factorization;bipartite graph;generic message-passing algorithm;computational rule;marginal functions;global function;artificial intelligence;signal processing;digital communications;forward/backward algorithm;Viterbi algorithm;iterative turbo decoding algorithm;belief propagation algorithm;Bayesian networks;Kalman filter;fast Fourier transform;FFT algorithms;HMM;Graph theory}, 
doi={10.1109/18.910572}, 
ISSN={0018-9448}, 
month={Feb},}
@article{10.2307/25651244,
 ISSN = {10618600},
 URL = {http://www.jstor.org/stable/25651244},
 abstract = {Hidden Markov random fields represent a complex hierarchical model, where the hidden latent process is an undirected graphical structure. Performing inference for such models is difficult primarily because the likelihood of the hidden states is often unavailable. The main contribution of this article is to present approximate methods to calculate the likelihood for large lattices based on exact methods for smaller lattices. We introduce approximate likelihood methods by relaxing some of the dependencies in the latent model, and also by extending tractable approximations to the likelihood, the so-called pseudolikelihood approximations, for a large lattice partitioned into smaller sublattices. Results are presented based on simulated data as well as inference for the temporal-spatial structure of the interaction between up-and down-regulated states within the mitochondrial chromosome of the Plasmodium falciparum organism. Supplemental material for this article is available online.},
 author = {N. Friel and A. N. Pettitt and R. Reeves and E. Wit},
 journal = {Journal of Computational and Graphical Statistics},
 number = {2},
 pages = {243--261},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
 title = {Bayesian Inference in Hidden Markov Random Fields for Binary Data Defined on Large Lattices},
 volume = {18},
 year = {2009}
}
@inproceedings{DBLP:journals/corr/KingmaW13,
  author    = {Diederik P. Kingma and
               Max Welling},
  title     = {Auto-Encoding Variational Bayes},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
               Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  year      = {2014},
  url       = {http://arxiv.org/abs/1312.6114},
  timestamp = {Thu, 04 Apr 2019 13:20:07 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/KingmaW13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@Article{erdos1960,
  Title                    = {On the evolution of random graphs},
  Author                   = {Erdos, Paul and R{\'e}nyi, Alfr{\'e}d},
  Journal                  = {Publ. Math. Inst. Hung. Acad. Sci},
  Year                     = {1960},
  Number                   = {1},
  Pages                    = {17--60},
  Volume                   = {5}
}
@book{James:2014:ISL:2517747,
 author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
 title = {An Introduction to Statistical Learning: With Applications in R},
 year = {2014},
 isbn = {1461471370, 9781461471370},
 publisher = {Springer Publishing Company, Incorporated},
} 
@inproceedings{Minka:2001:EPA:647235.720257,
 author = {Minka, Thomas P.},
 title = {Expectation Propagation for Approximate Bayesian Inference},
 booktitle = {Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence},
 series = {UAI '01},
 year = {2001},
 isbn = {1-55860-800-1},
 pages = {362--369},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=647235.720257},
 acmid = {720257},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 
@article{Ihler:2005:LBP:1046920.1088703,
 author = {Ihler, Alexander T. and Fischer III, John W. and Willsky, Alan S.},
 title = {Loopy Belief Propagation: Convergence and Effects of Message Errors},
 journal = {J. Mach. Learn. Res.},
 issue_date = {12/1/2005},
 volume = {6},
 month = dec,
 year = {2005},
 issn = {1532-4435},
 pages = {905--936},
 numpages = {32},
 url = {http://dl.acm.org/citation.cfm?id=1046920.1088703},
 acmid = {1088703},
 publisher = {JMLR.org},
} 
@inproceedings{Yedidia:2000:GBP:3008751.3008848,
 author = {Yedidia, Jonathan S. and Freeman, William T. and Weiss, Yair},
 title = {Generalized Belief Propagation},
 booktitle = {Proceedings of the 13th International Conference on Neural Information Processing Systems},
 series = {NIPS'00},
 year = {2000},
 location = {Denver, CO},
 pages = {668--674},
 numpages = {7},
 url = {http://dl.acm.org/citation.cfm?id=3008751.3008848},
 acmid = {3008848},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 
@inproceedings{Wiegerinck:2002:FBP:2968618.2968673,
 author = {Wiegerinck, Wim and Heskes, Tom},
 title = {Fractional Belief Propagation},
 booktitle = {Proceedings of the 15th International Conference on Neural Information Processing Systems},
 series = {NIPS'02},
 year = {2002},
 pages = {438--445},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=2968618.2968673},
 acmid = {2968673},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 
@phdthesis{Minka:2001:FAA:935427,
 author = {Minka, Thomas P.},
 title = {A Family of Algorithms for Approximate Bayesian Inference},
 year = {2001},
 note = {AAI0803033},
 publisher = {Massachusetts Institute of Technology},
 address = {Cambridge, MA, USA},
} 
@incollection{yingzhen2015sep,
title = {Stochastic Expectation Propagation},
author = {Li, Yingzhen and Hern\'{a}ndez-Lobato, Jos\'{e} Miguel and Turner, Richard E},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {2323--2331},
year = {2015},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5760-stochastic-expectation-propagation.pdf}
}
@inproceedings{Lin:2015:DLM:2969239.2969280,
 author = {Lin, Guosheng and Shen, Chunhua and Reid, Ian and Hengel, Anton van den},
 title = {Deeply Learning the Messages in Message Passing Inference},
 booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
 series = {NIPS'15},
 year = {2015},
 location = {Montreal, Canada},
 pages = {361--369},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2969239.2969280},
 acmid = {2969280},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}
@article{yoon2019inferenceGraph,
  author    = {KiJung Yoon and
               Renjie Liao and
               Yuwen Xiong and
               Lisa Zhang and
               Ethan Fetaya and
               Raquel Urtasun and
               Richard S. Zemel and
               Xaq Pitkow},
  title     = {Inference in Probabilistic Graphical Models by Graph Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1803.07710},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.07710},
  archivePrefix = {arXiv},
  eprint    = {1803.07710},
  timestamp = {Mon, 13 Aug 2018 16:48:16 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1803-07710},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{du2017convergenceBP,
 author = {Du, Jian and Ma, Shaodan and Wu, Yik-Chung and Kar, Soummya and Moura, Jos{\'e} M. F.},
 title = {Convergence Analysis of Distributed Inference with Vector-valued Gaussian Belief Propagation},
 journal = {J. Mach. Learn. Res.},
 issue_date = {January 2017},
 volume = {18},
 number = {1},
 month = jan,
 year = {2017},
 issn = {1532-4435},
 pages = {6302--6339},
 numpages = {38},
 url = {http://dl.acm.org/citation.cfm?id=3122009.3242029},
 acmid = {3242029},
 publisher = {JMLR.org},
 keywords = {Markov random field, graphical model, large-scale networks, linear gaussian model, walk-summability},
}
@article{Opper:2000:GPC:1121900.1121911,
 author = {Opper, Manfred and Winther, Ole},
 title = {Gaussian Processes for Classification: Mean-Field Algorithms},
 journal = {Neural Comput.},
 issue_date = {November 2000},
 volume = {12},
 number = {11},
 month = nov,
 year = {2000},
 issn = {0899-7667},
 pages = {2655--2684},
 numpages = {30},
 url = {http://dx.doi.org/10.1162/089976600300014881},
 doi = {10.1162/089976600300014881},
 acmid = {1121911},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}
@INPROCEEDINGS{jeon2015optimality, 
author={C. {Jeon} and R. {Ghods} and A. {Maleki} and C. {Studer}}, 
booktitle={2015 IEEE International Symposium on Information Theory (ISIT)}, 
title={Optimality of large MIMO detection via approximate message passing}, 
year={2015}, 
volume={}, 
number={}, 
pages={1227-1231}, 
keywords={message passing;MIMO communication;multiuser detection;large MIMO detection;approximate message passing;multiple-input multiple-output communication systems;wireless link;computational complexity;sub-optimal detection algorithms;novel data-detection method;AMP;Noise;MIMO;Phase shift keying;Message passing;Algorithm design and analysis;Compressed sensing;Quadrature amplitude modulation}, 
doi={10.1109/ISIT.2015.7282651}, 
ISSN={2157-8117}, 
month={June},}
@article{Pretti2005damping,
        doi = {10.1088/1742-5468/2005/11/p11008},
        url = {https://doi.org/10.1088%2F1742-5468%2F2005%2F11%2Fp11008},
        year = 2005,
        month = {nov},
        publisher = {{IOP} Publishing},
        volume = {2005},
        number = {11},
        pages = {P11008--P11008},
        author = {Marco Pretti},
        title = {A message-passing algorithm with damping},
        journal = {Journal of Statistical Mechanics: Theory and Experiment},
        abstract = {We propose a modified belief propagation algorithm, with over-relaxed dynamics. Such an
algorithm turns out to be generally more stable and faster than ordinary belief
propagation. We characterize the performance of the algorithm, employed as a tool for
combinatorial optimization, on the random satisfiability problem. Moreover, we trace a
connection with a recently proposed double-loop algorithm for minimizing Bethe and
Kikuchi free energies.}
@article{DBLP:journals/corr/abs-1106-0257,
  author    = {Richard Maclin and
               David W. Opitz},
  title     = {Popular Ensemble Methods: An Empirical Study},
  journal   = {CoRR},
  volume    = {abs/1106.0257},
  year      = {2011},
  url       = {http://arxiv.org/abs/1106.0257},
  archivePrefix = {arXiv},
  eprint    = {1106.0257},
  timestamp = {Mon, 13 Aug 2018 16:47:18 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1106-0257},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@Article{Rokach2010,
author="Rokach, Lior",
title="Ensemble-based classifiers",
journal="Artificial Intelligence Review",
year="2010",
month="Feb",
day="01",
volume="33",
number="1",
pages="1--39",
abstract="The idea of ensemble methodology is to build a predictive model by integrating multiple models. It is well-known that ensemble methods can be used for improving prediction performance. Researchers from various disciplines such as statistics and AI considered the use of ensemble methodology. This paper, review existing ensemble techniques and can be served as a tutorial for practitioners who are interested in building ensemble based systems.",
issn="1573-7462",
doi="10.1007/s10462-009-9124-7",
url="https://doi.org/10.1007/s10462-009-9124-7"
}
@incollection{opper2008imp_ep,
title = {Improving on Expectation Propagation},
author = {Opper, Manfred and Paquet, Ulrich and Winther, Ole},
booktitle = {Advances in Neural Information Processing Systems 21},
editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
pages = {1241--1248},
year = {2009},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/3613-improving-on-expectation-propagation.pdf}
}
@incollection{yingzhen2015stochastic_ep,
title = {Stochastic Expectation Propagation},
author = {Li, Yingzhen and Hern\'{a}ndez-Lobato, Jos\'{e} Miguel and Turner, Richard E},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {2323--2331},
year = {2015},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5760-stochastic-expectation-propagation.pdf}
}
